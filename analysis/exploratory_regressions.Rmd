---
title: "exploratory regressions"
author: "Brooke Staveland"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo <- FALSE,  # don't print the code chunk
  warning <- FALSE,  # don't print warnings
  message <- FALSE,  # don't print messages
  fig.width <- 12,  # set default width of figures
  fig.height <- 6,  # set default height of figures
  fig.align <- "center",  # always align figure in center
  fig.pos <- "H",  # always plot figure at the exact location of the code chunk
  cache <- TRUE)  # cache results

## libraries ##
library(tidyverse)
library(ggplot2)
library(magrittr)
library(ggthemr)
library(grid)
library(gtable)
library(gridExtra)
library(wesanderson)
library(ggsci)
library(zoo)
library(kableExtra)
library(lme4)
library(RColorBrewer)
library(lme4)
library(here)
library(fs)
library(ggcorrplot)

## hand written functions ##
source(path("R", "load_behave_data.R"))
source(path("R", "prep_behave_data.R"))
source('~/Projects/nice_r_functions/ggpaired_pretty.R')
source('~/Projects/nice_r_functions/mutate_cond.R')

```


This script is designed as the skeleton/testing ground for the regression scripts we will use in the main analyses, which will look for electrodes in which high gamma encodes different beavrioal measures from the dictator task. I will start by building some dummy data to play with.

```{r behave-prep}

# path to beahvioral files #
path_to_raw_behave <- fs::path(here(), "../dg_behave_formatted")
path_to_behave_munge <- fs::path(here(), "munge", "combined_behavioral_data.csv")
path_to_behave_clean <- fs::path(here(), "munge", "clean_behavioral_data.csv")

# concactenate behavioral data #
load_behave_data(path_to_raw_behave)

# prep behavioral data #
prep_behave_data(path_to_behave_munge)

# load clean data #
behave_data <- read.csv(path_to_behave_clean)

```

```{r behave_corrs}

behave_correl_data <- behave_data %>%
  filter(SID == "DG_s10") %>%
  select(-SID, -round, -ends_with("time"), -RT, -side_chosen, -X)

behavioral_cor <- cor(behave_correl_data, use = "pairwise.complete.obs")

ggcorrplot(behavioral_cor,
           lab = T,
           type = "lower",
           colors = wes_palette("Zissou1", 3, type = "continuous"))

```

I am wondering if it might make sense to start with the most derived var as the spotlight, ie disaventageous versus adventageous inequality.

```{r test-regress}

test_full <- lm(RT ~ self_diff + self_payoff + self_foregone, data <- behave_data)
summary(test_full)


test_part1 <- lm(RT ~ self_diff + self_payoff, data <- behave_data)
summary(test_part1)


test_part2 <- lm(RT ~ self_diff + self_foregone, data <- behave_data)
summary(test_part2)

###########


test_full <- lm(RT ~ self_var_payoff + other_var_payoff + ineq_advent, data <- behave_data)
summary(test_full)


test_part1 <- lm(RT ~ other_payoff + self_payoff + ineq_choice_abs, data <- behave_data)
summary(test_part1)


test_part2 <- lm(RT ~ self_diff + self_foregone, data <- behave_data)
summary(test_part2)


```

So if the derived vars are sum/subtraction from the other variables in the model, it will be over saturated and won't run. But advent/advent ineq are substantially different. 

```{r dummy-data}

fake_hg <- read.csv('./temp_data.csv', header <- F)

last_idx <- 1
fake_df <- data.frame()
for(idx in 1:2280){
  if(idx %% 10 == 0){
    temp_df <- fake_hg[1, last_idx:idx]
    colnames(temp_df) <- colnames(fake_df)
    fake_df <- rbind(fake_df, temp_df)
    last_idx <- idx + 1
  }
  
}

fake_df <- data.frame(fake_df)
row.names(fake_df) <- 1:228
colnames(fake_df) <- paste0("bin_", 1:10)

fake_behave <- behave_data %>% filter(SID == "DG_s10")

brain_behave_data_elec <- cbind(fake_df, fake_behave)

```


```{r regressions}

# save info needed for regressions #
niter <- 1000
nBins <- colnames(fake_df)
electrodes <- elec_notes$Electrode[!is.na(elec_notes$Electrode)]
electrodes <- electrodes[1:100]
# brain_behave_data is a df with nrow <- trials * elecs

# create results dfs #
results_advantageous <- data.frame(matrix(nrow = length(electrodes)*length(nBins), ncol = 7))
results_disadvantageous <- data.frame(matrix(nrow = length(electrodes)*length(nBins), ncol = 7))

colnames(results_advantageous) <- c("electrode", "bin", "R2", "absBeta", "Fstat", "p", "perm_p")
colnames(results_disadvantageous) <- c("electrode", "bin", "R2", "absBeta", "Fstat", "p", "perm_p")

# fill results dfs with electrode & bin info #
results_advantageous$electrode <- sort(rep(as.character(electrodes), length(nBins)))
results_advantageous$bin <- rep(nBins, length(electrodes))

results_disadvantageous$electrode <- sort(rep(as.character(electrodes), length(nBins)))
results_disadvantageous$bin <- rep(nBins, length(electrodes))


for (elec in electrodes) {
  
  # filter to single electrode #
  brain_behave_data_elec <- brain_behave_data %>% filter(electrode == elec)
  brain_behave_data_elec <- brain_behave_data_elec[complete.cases(brain_behave_data_elec), ]

  # initialize temp vars #
  r2_disadv <- NULL
  fstat_disadv <- NULL
  abs_beta_disadv <- NULL
  lm_pval_disadv <- NULL
  r2_adv <- NULL
  fstat_adv <- NULL
  abs_beta_adv <- NULL
  lm_pval_adv <- NULL
  
  for (bin in nBins) {
    # run models #
    bin_vec <- brain_behave_data_elec[, bin]
    model_disadvant <- summary(lm(bin_vec ~ ineq_disadvent, data = brain_behave_data_elec))
    model_advant <- summary(lm(bin_vec ~ ineq_advent, data = brain_behave_data_elec))
    
    # store info from models #
    r2_disadv[bin] <- model_disadvant$r.squared
    fstat_disadv[bin] <- model_disadvant$fstatistic[1]
    abs_beta_disadv[bin] <- abs(model_disadvant$coefficients[2, 1])
    lm_pval_disadv[bin] <- model_disadvant$coefficients[2,4]
    
    r2_adv[bin] <- model_advant$r.squared
    fstat_adv[bin] <- model_advant$fstatistic[1]
    abs_beta_adv[bin] <- abs(model_advant$coefficients[2,1])
    lm_pval_adv[bin] <- model_advant$coefficients[2,4]
  }
  
  # Find out the longest stretch in which pval < 0.05 and create sum-of-F-stats statistic
  stretch <- lm_pval < 0.05
  indices <- stretch_start_end(stretch)  # what does stretch_start_end do???
  
  # if there is no stretch, take the max beta and max f statistic #
  if(is.na(indices[1])) {
    beta_stretch_disadv <- max(abs_beta_disadv)
    fstat_stretch_disadv <- max(fstat_disadv)

    beta_stretch_adv <- max(abs_beta_adv)
    fstat_stretch_adv <- max(fstat_adv)

  # if there is a stretch, sum the betas and the f statistics #
  } else {
    beta_stretch_disadv <- sum(abs_beta_disadv[indices[1]:indices[2]]) # Summary stat
    fstat_stretch_disadv <- sum(fstat_disadv[indices[1]:indices[2]]) # Summary stat
    
    beta_stretch_adv <- sum(abs_beta_adv[indices[1]:indices[2]]) # Summary stat
    fstat_stretch_adv <- sum(fstat_adv[indices[1]:indices[2]]) # Summary stat    
  }
  
  # Create null distribution by shuffling labels #
  beta_stretch_null <- fstat_stretch_null <- fstat_null <- 0
  for(h in 1:niter) {
    abs_beta_null <- lm_pval_null <- 0
    for (bin in 1:nBins) { # This is the slooooow step
      
      # if(remove_outliers) {
      #   temp <- data.frame(peak=peak)
      #   temp <- remove_outliers(temp,peak)
      #   peak <- temp$peak
      # }
      
      # run models #
      null_lm_disadv <- summary(lm(bin ~ sample(ineq_disadvent)))
      null_lm_disadv <- summary(lm(bin ~ sample(ineq_disadvent)))
      
      # save info from models #
      null_fstat_disadv[i] <- null_lm_disadv$fstatistic[1]
      null_abs_beta_disadv[i] <- abs(null_lm_disadv$coefficients[2,1])
      null_lm_pval_disadv[i] <- null_lm_disadv$coefficients[2,4]
      
      null_fstat_adv[i] <- null_lm_adv$fstatistic[1]
      null_abs_beta_adv[i] <- abs(null_lm_adv$coefficients[2,1])
      null_lm_pval_adv[i] <- null_lm_adv$coefficients[2,4]      
    }
    
    # Find out the longest stretch in which pval < 0.05 and create sum-of-F-stats statistic
    stretch <- lm_pval_null < 0.05
    indices <- stretch_start_end(stretch)
    if(is.na(indices[1])) {
      # if no stretch just take the max vals #
      null_beta_stretch_disadv[h] <- max(null_abs_beta_disadv)
      null_fstat_stretch_disadv[h] <- max(null_fstat_disadv)

      null_beta_stretch_adv[h] <- max(null_abs_beta_adv)
      null_fstat_stretch_adv[h] <- max(null_fstat_adv)

      } else {
      # if a stretch take the sum of f stats #  
      null_beta_stretch_disadv[h] <- sum(null_abs_beta_disadv[indices[1]:indices[2]]) # Summary stat
      null_fstat_stretch_disadv[h] <- sum(null_fstat_disadv[indices[1]:indices[2]]) # Summary stat
      
      null_beta_stretch_adv[h] <- sum(null_abs_beta_adv[indices[1]:indices[2]]) # Summary stat
      null_fstat_stretch_adv[h] <- sum(null_fstat_adv[indices[1]:indices[2]]) # Summary stat

    }
    # if(fstat_stretch_null[h] > 140) { break }
  }
  
  # our test statistic for the permutation test is the sum of f tests #
  perm_pval_disadv <- sum(null_fstat_stretch_disadv > fstat_stretch_disadv)/niter
  perm_pval_adv <- sum(null_fstat_stretch_adv > fstat_stretch_adv)/niter
  
  # save vals in results df #
  results_advantageous <- results_advantageous %>% 
    mutate_cond(electrode == elec, R2 = r2_adv, absBeta = abs_beta_adv, Fstat = fstat_adv, p = lm_pval_adv, perm_p = perm_pval_adv)
    
  results_disadvantageous <- results_disadvantageous %>% 
    mutate_cond(electrode == elec, R2 = r2_disadv, absBeta = abs_beta_disadv, Fstat = fstat_disadv, p = lm_pval_disadv, perm_p = perm_pval_disadv)


}

# save results to results folder #
write.csv(results_disadvantageous, path(here(), "results", "results_disadvantageous.csv"))
write.csv(results_advantageous, path(here(), "results", "results_advantageous.csv"))


```


# Prediction HG -> choice

maybe after regions are identified in first couple of subjects so it can be truly uniased in the later subjects

